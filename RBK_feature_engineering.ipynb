{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('dataset/train_dataset_train_full_proc.parquet')\n",
    "df_test = pd.read_parquet('dataset/test_dataset_test_full_proc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_categories = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_category'] = embedding_categories.fit_transform(df_train[['category']]).todense().tolist()\n",
    "df_test['embedding_category'] = embedding_categories.transform(df_test[['category']]).todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_category.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_categories, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document_id_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_document_id_1'] = embedding_document_id_1.fit_transform(df_train[['document_id_1']]).todense().tolist()\n",
    "df_test['embedding_document_id_1'] = embedding_document_id_1.transform(df_test[['document_id_1']]).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_document_id_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_document_id_1, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document_id_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_document_id_2'] = embedding_document_id_2.fit_transform(df_train[['document_id_2']]).todense().tolist()\n",
    "df_test['embedding_document_id_2'] = embedding_document_id_2.transform(df_test[['document_id_2']]).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_document_id_2 .pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_document_id_2 , handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document_id_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_document_id_3'] = embedding_document_id_3.fit_transform(df_train[['document_id_3']]).todense().tolist()\n",
    "df_test['embedding_document_id_3'] = embedding_document_id_3.transform(df_test[['document_id_3']]).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_document_id_3', 'wb') as handle:\n",
    "    pickle.dump(embedding_document_id_3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document_id_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_document_id_4'] = embedding_document_id_4.fit_transform(df_train[['document_id_4']]).todense().tolist()\n",
    "df_test['embedding_document_id_4'] = embedding_document_id_4.transform(df_test[['document_id_4']]).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_document_id_4.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_document_id_4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document_id_5 = OneHotEncoder(handle_unknown='ignore')\n",
    "df_train['embedding_document_id_5'] = embedding_document_id_5.fit_transform(df_train[['document_id_5']]).todense().tolist()\n",
    "df_test['embedding_document_id_5'] = embedding_document_id_5.transform(df_test[['document_id_5']]).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_document_id_5.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_document_id_5, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tag = CountVectorizer(min_df=12)\n",
    "vectorizer_tag.fit(df_train['tags'].tolist())\n",
    "df_train['embedding_tags'] = df_train['tags'].apply(lambda x: vectorizer_tag.transform([x]).todense().tolist()[0])\n",
    "df_test['embedding_tags'] = df_test['tags'].apply(lambda x: vectorizer_tag.transform([x]).todense().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_tags_vectorize.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer_tag, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_author = CountVectorizer(min_df=12)\n",
    "vectorizer_author.fit(df_train['authors'].tolist())\n",
    "df_train['embedding_authors'] = df_train['authors'].apply(lambda x: vectorizer_author.transform([x]).todense().tolist()[0])\n",
    "df_test['embedding_authors'] = df_test['authors'].apply(lambda x: vectorizer_author.transform([x]).todense().tolist()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_authors_vectorize.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer_author, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_titles = CountVectorizer(min_df=12)\n",
    "df_train['embedding_title'] = vectorizer_titles.fit_transform(df_train['title'].tolist()).todense().tolist()\n",
    "df_test['embedding_title'] = vectorizer_titles.transform(df_test['title'].tolist()).todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_titles_vectorize.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer_titles, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_fulltext = CountVectorizer(min_df=200)\n",
    "df_train['embedding_full_text'] = vectorizer_fulltext.fit_transform(df_train['full_text'].tolist()).todense().tolist()\n",
    "df_test['embedding_full_text'] = vectorizer_fulltext.transform(df_test['full_text'].tolist()).todense().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/embedding_fulltext_vectorize.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer_fulltext, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"DeepPavlov/rubert-base-cased\"\n",
    "    max_len = 512\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBKDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{CFG.model}\")\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.full_text_original.tolist()\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.views = torch.tensor(df.views, dtype=torch.float32)  \n",
    "            self.depth = torch.tensor(df.depth, dtype=torch.float32) \n",
    "            self.full_reads_percent = torch.tensor(df.full_reads_percent, dtype=torch.float32)  \n",
    "    \n",
    "        self.encoded = self.tokenizer.batch_encode_plus(\n",
    "            self.text, \n",
    "            padding = 'max_length', \n",
    "            max_length = CFG.max_len,\n",
    "            add_special_tokens=True,\n",
    "            truncation = True,\n",
    "            return_attention_mask = True\n",
    "        )\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }           \n",
    "        else:\n",
    "            views = self.views[index]\n",
    "            depth = self.depth[index]\n",
    "            full_reads_percent = self.full_reads_percent[index]\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'target': torch.stack([views, depth, full_reads_percent], dim=0)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBKModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(f\"{CFG.model}\")\n",
    "        self.rubert = AutoModel.from_pretrained(f\"{CFG.model}\", config=self.config)  \n",
    "        self.attention = nn.Sequential(            \n",
    "                                            nn.Linear(768, 128),            \n",
    "                                            nn.Tanh(),                       \n",
    "                                            nn.Linear(128, 1),\n",
    "                                            nn.Softmax(dim=1)\n",
    "                                        )\n",
    "  \n",
    "        self.fc1 = nn.Linear(self.config.hidden_size, 100)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        rubert_output = self.rubert(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)\n",
    "        weights = self.attention(rubert_output['last_hidden_state'])\n",
    "        context_vector = torch.sum(weights * rubert_output['last_hidden_state'], dim=1)\n",
    "        fc1 = self.fc1(context_vector)\n",
    "        logits = self.fc2(self.relu(self.dropout(fc1)))\n",
    "        return {'logits': logits,\n",
    "               'context_vector': fc1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_BERT(df):\n",
    "    loader = DataLoader(\n",
    "        RBKDataset(df.reset_index(), inference_only=True), \n",
    "        batch_size=1, \n",
    "        shuffle=False\n",
    "    )\n",
    "    prediction = np.zeros((4, len(df), 100))\n",
    "    for number_model, model_path in enumerate(glob.glob('models_bert/*.pth')):\n",
    "        model = RBKModel().to(CFG.device)\n",
    "        model.load_state_dict(torch.load(f\"{model_path}\"))\n",
    "        model.eval()\n",
    "        for index, batch in enumerate(loader):\n",
    "            pred = model(\n",
    "                batch['input_ids'].to(CFG.device), \n",
    "                batch['attention_mask'].to(CFG.device)\n",
    "            )['context_vector']\n",
    "            prediction[number_model, index, :] = pred.cpu().detach().numpy()\n",
    "    return np.mean(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 45s, sys: 1.44 s, total: 6min 46s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['bert_embedding'] = get_predict_BERT(df_train).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['bert_embedding'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df_test['bert_embedding'] = get_predict_BERT(df_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('dataset/train_dataset_train_full_proc.parquet')\n",
    "df_test.to_parquet('dataset/test_dataset_test_full_proc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
